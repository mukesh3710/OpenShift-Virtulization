### OpenShift Networking
---
SDN:
- OpenShift uses a software-defined networking (SDN) model to manage communication between pods, services, and nodes. When you install OpenShift (using IPI or UPI), you configure three key networks (Cluster Network, Machine Network & Service Network)
- Cluster Network: Internal network for Pods to communicate with each other across nodes. Every Pod gets an IP from this range, and SDN/OVN-Kubernetes handles Pod A on Node1 → Pod B on Node2 uses Cluster Network (routing.CIDR: 10.128.0.0/14)
- Machine Network: Network for the nodes themselves (control plane and worker nodes) to communicate with each other and with the outside world. Set during install; must include all node IPs. 
- Service Network: Virtual network for Kubernetes Services (ClusterIP) to expose applications internally. Pods talk to Services via this network using kube-proxy.
- How They Work Together
  - Each node connects to OVS bridge (br-ex) linked to Machine Network.
  - Pods/VMs get IP from Cluster Network (overlay via Geneve tunnels).
  - Services provide load balancing + DNS inside cluster.
---
Network Definition:
- Machine Network: External network for node IPs, DNS, and gateway.
- Bonding for High Availability: Combine multiple NICs into LACP active-active bond for redundancy and performance. Example: bond0 aggregates eth0 + eth1, both connected to the same VLAN.
- Bridge Configuration: br-ex: Attached to bond0, gets Machine Network IP (and sometimes VIP for API access). Integration bridge: Connects br-ex to OVN overlay network for Pod networking.
- Cluster Network Interface: ovn-k8s-mp0: Virtual interface for Cluster Network (e.g., 10.128.0.5). Used for Pod traffic inside cluster. Ping works between nodes over Geneve tunnels, not from outside cluster.
- Routing: Cluster Network traffic routes internally via gateways, but not exposed externally. Example: traceroute from Node1 to Node2’s Pod IP shows multiple internal hops.
---
Underlay vs Overlay:
- Underlay Network: Physical network using VLANs (e.g., 192.168.122.0/24) for node connectivity. VLAN 100 for nodes (physical VLAN).
- Overlay Network: Virtual network (e.g., Geneve or VXLAN) on top of underlay for Pods/VMs communication. Scalable, avoids VLAN limits. Cluster Network 10.128.0.0/14 for Pods via Geneve tunnels (Cluster Network).
---
Additional Interfaces (Data Network): 
- You can add another physical interface → create OVS bridge (br-data) → attach VLANs for advanced networking (via NetworkAttachmentDefinition CRDs). Example: VM on VLAN 10 gets IP 192.168.10.x and is accessible externally without NAT.
- Add second NIC → create OVS bridge (br-data) → make it a trunk port to carry VLANs (e.g., VLAN 10 & 20). Use NetworkAttachmentDefinition CRDs for Pods/VMs to connect to these VLANs. VM on VLAN 10 gets IP 192.168.10.50, No NAT needed, VM is directly accessible via router IP 192.168.68.105.
- Security and Microsegmentation: OVN allows applying ACLs and security policies even for external networks.
- Performance Boost: For high performance, bypass OVS bridge using MACVLAN, SR-IOV, or passthrough devices to Pods/VMs.
---
Pod and Service Networking: 
- Pods: Each Pod gets an IP from the Cluster Network (e.g., 10.128.0.12). Pods can directly talk to each other across nodes. Example: Pod A (10.128.0.12) → Pod B (10.128.1.8)
- Services: A Service gets a virtual IP from the Service Network (e.g., 172.30.100.50) and acts as an internal load balancer for Pods. DNS (e.g., myapp.default.svc.cluster.local) resolves to this IP. Example: Pod A → Service (172.30.100.50) → routes to backend Pods.
- Why Service Network Exists: Pod IPs are ephemeral, so Service Network provides stable IP and DNS for apps. Example: myapp.default.svc.cluster.local → 172.30.100.50.
---
External Access: 
- NodePort: Exposes service on each node’s IP at a static port (e.g., NodeIP:30080).
Real-world: Production app on AWS that needs direct internet access with auto-scaling support.
- LoadBalancer: Uses external load balancer (cloud or MetalLB) for single IP. Kubernetes Service type that provisions a cloud load balancer (e.g., AWS ELB, Azure LB, GCP LB). Exposes app on a dedicated public IP. Direct external access to a service with automatic load distribution. Eg: kubectl expose deployment myapp --type=LoadBalancer → gets public IP 34.120.x.x.
- Ingress/Route (OpenShift): Kubernetes object that manages external HTTP/HTTPS access to services using a single entry point .Provides HTTP/HTTPS access via DNS. Centralized way to expose multiple apps on the same domain with path-based or host-based routing. Real-world: SaaS platform hosting multiple microservices behind one domain. (e.g., https://myapp.apps.example.com).
- Route (OpenShift-specific): OpenShift’s way of exposing a service externally. Similar to Ingress but simpler and tightly integrated with OpenShift’s router. Expose an app quickly without writing an Ingress definition. Real-world: Dev team wants to test their app externally with minimal config. Eg: oc expose service myapp → creates route http://myapp.apps.cluster.example.com.
---
Securing Pods with NetworkPolicy:
- Controls Pod-to-Pod traffic inside the Cluster Network. Uses labels (similar to Services) to define which Pods can talk to which. By default, all Pods can communicate with each other. Apply a NetworkPolicy to deny all traffic. You can allow traffic only from specific Pods, Namespaces, or Ports using labels.
---
Example:
- Pod Deployment: Two BusyBox Pods deployed on different nodes. Each Pod gets an IP from Cluster Network. Pod1 runs a web server on port 8000 showing its IP. Pod2 can access Pod1 via Cluster Network → success.
- Service Creation: Create a Service to load balance traffic between Pods. Service gets an IP from Service Network (Service IP: 172.30.200.50 - Type: ClusterIP internal only). Add label to both Pods so the Service can select them.
- Access Service from Pod1 using curl 172.30.200.50:8000. Responses alternate between Pod1 and Pod2 (round-robin load balancing).
- Exposing Service Externally: Ingress Controller + Route is used to make the Service accessible from outside the cluster. Create an OpenShift Route for the BusyBox Service. OpenShift generates a URL (e.g., http://busybox.apps.example.com) that maps to the Service.
- Limitations: Routes only support HTTP/HTTPS (ports 80, 443). For non-HTTP protocols (e.g., databases, RDP), use MetalLB or LoadBalancer Service.
---
---
Networking for OpenShift Virtualization:
- Default Networking for OpenShift Virtualization (CNV/KubeVirt): When you create a VM in OpenShift with default settings, the VM gets a Pod network IP (just like a pod). Inside the VM, if you check the IP, you’ll always see something like 10.0.2.2. This is not a duplicate conflict. OpenShift is doing NAT (like how Docker maps 172.x.x.x to containers). You can still ping the VM using its pod IP, and Kubernetes services/routes work the same way as with pods.
- VLAN / Secondary Network (Direct Access without NAT): If you don’t want NAT and want VMs to behave like on VMware or bare metal with VLAN IPs.
  - Bridge Mapping: Map a local bridge (e.g., br0) on the node to a VLAN. Tell OpenShift to use it as a "local network". VLAN 1105 bridged to br0.
  - NetworkAttachmentDefinition (NAD): Create a NAD in OpenShift that points to this bridge & VLAN.
  - VM Deployment: In the VM spec, replace Pod network with this new VLAN network. Provide static IP via cloud-init or let DHCP/your IPAM assign it. VM spec attached to VLAN 1105. Cloud-init gives VM IP 192.168.110.50.
---
Layer 2 Secondary Networks in OpenShift Virtualization:
- These are like OpenStack tenant networks - Internal / private networks (non-routable). Teams/projects can create their own isolated networks. Support overlapping IP ranges (Team A and Team B can both use 192.168.1.0/24). No bridge mapping needed → it’s an overlay network managed by OpenShift.
- How it Works: Create a Layer 2 network (just define a network name + namespace/project). Attach this network to your VM (instead of Pod network or VLAN network). VM gets an IP from that isolated network. Traffic stays private unless you explicitly connect it to a router or another network.
---
Microsegmentation: 
- For Pod networks → we use NetworkPolicies.
- For Secondary networks (like L2 overlays or VLANs) → we use MultiNetworkPolicies. Define rules for ingress/egress per secondary network.
- L2 network = team-private-net. MultiNetworkPolicy allows only SSH (port 22) between VMs in the same namespace. Blocks all other traffic.
